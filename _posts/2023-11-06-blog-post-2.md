---
title: 'Variational Autoencoder Note'
date: 2023-11-06
permalink: /posts/2023/11/blog-post-2/
tags:
  - statistical methods
  - core ML
  - boring stuff
---

This is a quick note on **Variational Autoencoders** freely inspired by Kevin Murphy's reference book *Advanced Topics for Probabilistic Machine Learning*. Variational Autoencoders are a useful tool box that emerge from adding a "probabilistic layer" to the autoencoder architecture from core ML. 

# Introduction

To introduce variational auto-encoders, two aspects need clarifying:
- autoencoders are a particular family of neural networks architecture 
- variational inference coming from the bayesian idea of approximating posterior through optimisation on a family of flexible distributions.

## Auto-encoders
What is an autoencoder? It is usually defined as a type of neural network that decomposes into an **encoding** component and a **decoding** component where in the former we encode the input $\mathcal{X}=\mathbb{R}^n$ into a smaller dimensioned representation called *latent representation* lying in space $\mathbb{R}^m, \ m\leq n$, and where in the latter we try to recover the input through the decoder. Since the architecture learns to recover information, it can also generate new data similar to its training input and belongs to the class of generative models at large. 

![Deterministic Autoencoder Architecture](/claire-stats.github.io/images/bp-autoencoder-deterministic.png)

Usually we define the encoder with a MLP (multilayer perceptron) $f$ such that $z = f(x) \in \mathbb{R}^m$ would be the representation of $x$ and the decoder with another MLP $g$ such that the reconstruction $x = g(z) \in \mathbb{R}^n$. The encoder is therefore trained by minimising the reconstruction loss to recover our input as much as we can $L(f,g)=\mathbb{E} \mid \mid x-g(f(x))\mid\mid_{2}^{2}$. 

A quick look at the mathematical formulation: if $m \geq n$, then we might learn the identity map in $f$ and $g$ which is not useful, setting $m \leq n$ is setting the problem in an under-complete setting and forces the learned mappings to be non trivial.

But why would one want to use an auto-encoder? For someone only just exposed to Machine Learning and focusing mostly in prediction or classification tasks, they might lean towards trying more complex and flexible models, therefore it might seem counter intuitive to try to reduce dimensionality of the problem. However there are perks to compressing the data into a smaller dimension, and one can see this idea of autoencoder architecture as a non linear extension to PCA. By taking "most of the information", we can leave out noisy channels, and help reconstruction (see Information Retrieval). But by compressing the problem, we can also learn useful representations: a more straightforward example of useful representation learning is the NMF (non negative matrix factorization) application to facial recognition where reconstruction can be achieved learning "parts" of the face (mouth, eyebrow, nose...) instead of each pixel. This allows less noise injection than pixel-by-pixel methods and less sensitivity towards randomized attacks (if you flip pixels one by one at random in the training set). This makes it a powerful tool to unlock patterns as well as to generate patterns. 

## Variational Inference

If variational autoencoders have architectural similarities to deterministic autoencoders, the theoretical problem is in fact very different. VAE belong to a class of Bayesian variational methods. What do variational methods do? 
Assuming that our observed variable $x$ can be modeled through a latent (unknown) process $z$, and parameters $\theta$, we model the **relationship** between those variables through their probabilistic **distributions**. 

In a Bayesian setting, one might assume :
- a prior $z \sim p_{\theta}(z)$
- observe the likelihood of the data $p_{\theta}(x\mid z)$ 
- then want to infer the posterior of our data generating process $p_{\theta}(z \mid x) = \frac{p_{\theta}(x,z)}{p_{\theta}(x)} = \frac{p_{\theta}(x,z)}{\int p_{\theta}(x,z) dz}$

The posterior might not be computationally tractable (it's hard to integrate quantities!).

![Variational inference approximation of posterior distribution](/claire-stats.github.io/images/bp-VI.png)

Variational inference  tries to approximate the posterior through an optimisation framework where we sift through a family of flexible enough distributions to get as close to the true posterior as we can. This notion of distance between two distributions is given by the KL divergence : $\min_{q_{\psi} \in \mathcal{Q}} KL(q_{\psi}(z) \mid\mid p_{\theta}(z\mid x))$. 
By some computations, we can rewrite this minimisation problem as maximising a bound called ELBO (evidence lower bound) that can be interpreted as maximising the expected log likelihood (learn from what we see) while minimising the distance between posterior to prior (we do not want to diverge from the prior too much).  

$$\begin{eqnarray*}
KL(q_{\psi}(z)||p_{\theta}(z|x)) & = & \mathbb{E}_{z \sim q_\psi} \Bigg( \log q_{\psi}(z)- \log \frac{p_{\theta}(x|z)p_\theta(z)}{p_\theta(x)} \Bigg) \\ 
& = & \log p_\theta(x) + E_{z\sim q_\psi} \log q_\psi (z) - \log p_\theta(x|z) - \log p_\theta(z) \\
& \geq &  E_{z\sim q_\psi} \log q_\psi (z) - \log p_\theta(x|z) - \log p_\theta(z)  \\
& = &  E_{z\sim q_\psi} \bigg( \log q_\psi (z) - \log p_\theta(x,z)  \bigg)
\end{eqnarray*}
$$
$$ \begin{eqnarray*}
KL(q_{\psi}(z)||p_{\theta}(z|x)) & \geq & 0 \\
\Rightarrow \log p_\theta(x) & \geq & - \bigg( E_{z\sim q_\psi} \bigg( \log q_\psi (z) - \log p_\theta(x,z) \bigg)\bigg) =: \mathtt{ELBO}
\end{eqnarray*}
$$ 
 Many methods derive from this principle:
 - stochastic variational inference (**SVI**): use a stochastic approximation for the ELBO to simplify the objective (faster to compute) by taking a random minibatch $B$ from the dataset and $\mathtt{ELBO}((\psi_{n}), \theta) \approx \frac{N}{B} \sum_{x_n \in B} \mathtt{ELBO}(\psi_n, \theta)$. 
 - amortized variational inference (**amortized VI**): instead of optimising to find $\psi_n$ at each $n$ in the minibatch, we use an *inference network* that is trained to predict $\psi_n$ from $x_n$ such that $\psi_{n} = f_{\phi}^{inf}(x_{n})$ 
 
 This can be combined with SVI. This is the framework in which VAE belong mostly.
 - gradient-based VI: if $q$ is chosen so that the ELBO is differentiable then we can use MC samples after pushing gradients into expectation to estimate the gradients and minimise through SGD.
 - reparametrized VI: as an extension of the gradient-based idea, we reparametrise the $q$ so that we can differentiate easily on the parameter and sample independently therefore gradients can be propagated back since we can switch gradient/expectation. 
 - mean field VI: Gaussian based, sometimes can be analytically optimised sometimes using rVI...
 - coordinate ascent VI, ADVI and more (see Murphy)
 
# Variational Auto Encoders

![Variational Autoencoders](/claire-stats.github.io/images/bp-VAE.png)

## In Theory

Assumptions on the generative model:
- Prior $z \sim p_{\theta}(z)$ 
- Likelihood of observed data e.g. $p_{\theta}(x \mid z) = \prod_{i=1}^n p_{\theta}(x_i \mid z_i)$ 
- A family of distribution for the observed data (usually exponential family) $p_{\theta}(x_i \mid z_i)$

The VAE estimates the parameters $\theta$ using a decoder neural network $d_{\theta}(z)$ for which $p_{\theta}(x_i \mid z_i) = g(x \mid d_{\theta}(z))$ 

and a "recognition" model (amortized inference) $e_{\phi}(x)$ to guide the approximate posterior $q_{\phi}(z\mid x) = q(z \mid e_{\phi}(x)) \approx p_{\theta}(z \mid x)$. 

Usually, we can take $q_{\phi}(z \mid x)$ 

to be a Gaussian in most cases so that we can use a reparametrization trick  on $z = \mu + \exp(\nu)\epsilon$ where $\epsilon \sim N(0,I)$ independently, in order to be able to sample whilst applying stochastic gradient on the parameters. 

Therefore the encoder needs to learn two parameters: the mean and the variance: $q_{\phi}(z \mid x)=N(z\mid \mu, \exp(\nu) I)$ and $e_{\phi}(x) = (\hat{\mu}, \hat{\nu})$. 

After initialising/updating all these parameters and sampling our $z$ and $x$ from the distributions, the amortized variational inference framework needs maximising the ELBO as mentioned before. In particular, we can write this as the following.



## In Practice

Algorithm pseudo-code where we take $q$ as before:
` 
- Initialise parameters $\theta, \phi$
- Repeat until convergence:
    - sample $x \sim p_{\theta}(x|z)$ 
    - sample $\epsilon \sim N(0,I)$ 
    - learn $\phi$ through the MLP $e_{\phi}(x)$
    - reconstruct $z$ from $\epsilon$ and $\phi$
    - learn $\theta$ through the MLP $d_{\theta}(z)$
    - Compute ELBO approximation and minimise it through stochastic gradient descent of the parameters
    - Update parameters 
`
We can understand the VAE in practice as alternating between the sampling processes and the optimisation processes. 

======
