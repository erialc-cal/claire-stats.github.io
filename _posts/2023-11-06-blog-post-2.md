---
title: 'Variational Autoencoder Note'
date: 2023-11-06
permalink: /posts/2023/11/blog-post-2/
tags:
  - statistical methods
  - core ML
  - boring stuff
---

This is a quick note on Variational Autoencoders freely inspired by Kevin Murphy's reference book *Advanced Topics for Probabilistic Machine Learning*. Variational Autoencoders are a useful tool box that emerge from adding a "probabilistic layer" to the the autoencoder architecture one might be more familiar with when coming from core ML. We will see how that probabilistic formulation changes from the deterministic perspective and if it allows strengths or weaknesses. 

# Introduction

To introduce variational auto-encoders, one has to explain both:
- autoencoders: a particular family of neural networks architecture 
- variational inference: the bayesian idea of approximating posterior through optimisation on a family of flexible distributions

## Auto-encoders
What is an auto-encoder? It is usually defined as a type of neural network that decomposes into an **encoding** component and a **decoding** component where we encode the input $\mathcal{X}=\mathbb{R}^n$ into a smaller dimensioned representation called *latent representation*  $\mathbb{R}^m, \ m\leq n$ in the encoding part and where we try to recover the input in the decoding part. Since the architecture learns to recover information, it can also generate new data similar to its input and belongs to the class of generative models at large. 
<br/><img src='./images/bp-autoencoder-deterministic.png'>
Usually we define the encoder with a MLP $f$ such that $\hat{z} = f(x) \in \mathbb{R}^m$ would be the representation of $x$ and the decoder with a MLP $g$ such that the reconstruction $\hat{x} = g(\hat{z}) \in \mathbb{R}^n$. The encoder is therefore trained by minimising the reconstruction loss $L(f,g)=\mathbb{E} ||x-g(f(x))||_2^2 $ in order to recover our input as much as we can. 

But why would one want to use an auto-encoder? For someone only just exposed to Machine Learning and focusing mostly in prediction or classification tasks, they might lean towards trying more complex and flexible models, therefore it might seem counter intuitive to try to reduce dimensionality of the problem. However there are perks to compressing the data into a smaller dimension, and one can see this idea of autoencoder architecture as a non linear extension to PCA. By taking "most of the information", we can leave out noisy channels, and help reconstruction (see Information Retrieval). But by compressing the problem, we can also learn useful representations: a useful metaphor is the NMF (non negative matrix factorization) application to facial recognition where reconstruction can be achieved learning "parts" of the face (mouth, eyebrow, nose...) instead of each pixel, this allows less noise injection than pixel-by-pixel methods and less sensitivity towards randomized attacks (if you flip pixels one by one at random in the training set). This makes it a powerful tool to unlock patterns. 

If variational autoencoders have architectural similarities to deterministic autoencoders, the theoretical problem is in fact very different. VAE belong to a class of Bayesian variational methods. What do variational methods do? 
Assuming that our observed variable $x$ can be modeled through a latent (unknown) process $z$, and parameters $\theta$, we model the **relationship** between those variables through their probabilistic **distributions**. In a Bayesian setting, one might assume $z \sim p_\theta(z)$ prior and  observe $p_\theta(x|z)$ the likelihood of the data, then the posterior of our data generating process is $p_\theta(z|x) = p_\theta(x,z)/p_\theta(x) = \frac{p_\theta(x,z)}{\int p_\theta(x,z) dz}$ then the posterior might not be computationally tractable (it's hard to integrate quantities!). Then variational inference  tries to approximate the posterior through an optimisation framework where we sift through a family of flexible enough distribution to get as close to the true posterior as we can. This notion of distance between two distributions is given by the KL divergence : $\min_{q_\psi \in \mathcal{Q}} KL(q_\psi(z)||p_\theta(z|x))$. By some computations, we can rewrite this minimisation problem as a maximising a bound called ELBO (evidence lower bound) that can be interpreted as maximising the expected log likelihood (learn from what we see) while minimising the distance between posterior to prior (we do not want to diverge from the prior too much).  
 Many methods derive from this principle:

 - stochastic variational inference (**SVI**): use a stochastic approximation for the ELBO to simplify the objective (faster to compute) by taking a random minibatch $B$ from the dataset and $ELBO((\psi_n)_{n}, \theta) \approx \frac{N}{B} \sum_{x_n \in B} ELBO(\psi_n, \theta)$. 
 - amortized variational inference (**amortized VI**): instead of optimising to find $\psi_n$ at each $n$ in the minibatch, we use an *inference network* that is trained to predict $\psi_n$ from $x_n$ such that $\psi_n = f_\phi^{inf}(x_n)$. This can be combined with SVI. This is the framework in which VAE belong mostly.
 - gradient-based VI: if $q$ is chosen so that the ELBO is differentiable then we can use MC samples after pushing gradients into expectation to estimate the gradients and minimise through SGD.
 - reparametrized VI: as an extension of the gradient-based idea, we reparametrise the $q$ so that we can differentiate easily on the parameter and sample independently therefore gradients can be propagated back since we can switch gradient/expectation. 
 - mean field VI: Gaussian based, sometimes can be analytically optimised sometimes using rVI...
 - CAVI
 - ADVI
 
# Variational Auto Encoders





## In Theory


Aren't headings cool?


## In Practice
======
