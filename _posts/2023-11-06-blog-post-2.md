---
title: 'Variational Autoencoder Note'
date: 2023-11-06
permalink: /posts/2023/11/blog-post-2/
tags:
  - statistical methods
  - core ML
  - boring stuff
---

This is a quick note on Variational Autoencoders freely inspired by Kevin Murphy's reference book *Advanced Topics for Probabilistic Machine Learning*. Variational Autoencoders are a useful tool box that emerge from adding a "probabilistic layer" to the the autoencoder architecture one might be more familiar with when coming from core ML. We will see how that probabilistic formulation changes from the deterministic perspective and if it allows strengths or weaknesses. 

# Introduction
======
To introduce variational auto-encoders, one has to start with a two sided coin:
- autoencoders: a particular family of neural networks architecture 
- variational inference: the bayesian idea of approximating posterior through optimisation on a family of flexible distributions

## Auto-encoders
What is an auto-encoder? It is usually defined as a type of neural network that decomposes into an **encoding** component and a **decoding** component where we encode the input $\mathcal{X}=\mathbb{R}^n$ into a smaller dimensioned representation called *latent representation*  $\mathbb{R}^m, \ m\leq n$ in the encoding part and where we try to recover the input in the decoding part. Since the architecture learns to recover information, it can also generate new data similar to its input and belongs to the class of generative models at large. 
<br/><img src='/images/bp-autoencoder-deterministic.png'>
Usually we define the encoder with a MLP $f$ such that $\hat{z} = f(x) \in \mathbb{R}^m$ would be the representation of $x$ and the decoder with a MLP $g$ such that $\hat{x} = g(\hat{z}) \in \mathbb{R}^n$. The encoder is then trained by minimising the reconstruction loss $L(f,g)=\mathbb{E} ||x-g(f(x))||_2^2 $
 
# Variational Auto Encoders
======

## In Theory
======

Aren't headings cool?


## In Practice
======
