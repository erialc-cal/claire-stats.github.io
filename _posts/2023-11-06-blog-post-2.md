---
title: 'Variational Autoencoder Note'
date: 2023-11-06
permalink: /posts/2023/11/blog-post-2/
tags:
  - statistical methods
  - core ML
  - boring stuff
---

This is a quick note on **Variational Autoencoders** freely inspired by Kevin Murphy's reference book *Advanced Topics for Probabilistic Machine Learning*. Variational Autoencoders are a useful tool box that emerge from adding a "probabilistic layer" to the autoencoder architecture from core ML. 

# Introduction

To introduce variational auto-encoders, two aspects need clarifying:
- autoencoders are a particular family of neural networks architecture 
- variational inference coming from the bayesian idea of approximating posterior through optimisation on a family of flexible distributions.

## Auto-encoders
What is an autoencoder? It is usually defined as a type of neural network that decomposes into an **encoding** component and a **decoding** component where in the former we encode the input $\mathcal{X}=\mathbb{R}^n$ into a smaller dimensioned representation called *latent representation* lying in space $\mathbb{R}^m, \ m\leq n$, and where in the latter we try to recover the input through the decoder. Since the architecture learns to recover information, it can also generate new data similar to its training input and belongs to the class of generative models at large. 

<img src='./images/bp-autoencoder-deterministic.png' alt='Deterministic Autoencoder Architecture' />

Usually we define the encoder with a MLP (multilayer perceptron) $f$ such that $z = f(x) \in \mathbb{R}^m$ would be the representation of $x$ and the decoder with another MLP $g$ such that the reconstruction $x = g(z) \in \mathbb{R}^n$. The encoder is therefore trained by minimising the reconstruction loss $L(f,g)=\mathbb{E} ||x-g(f(x))||_{2}^{2}$ in order to recover our input as much as we can. A quick look at the mathematical formulation: if $m \geq n$, then we might learn the identity map in $f$ and $g$ which is not useful, setting $m \leq n$ is setting the problem in an under-complete setting and forces the learned mappings to be non trivial.

But why would one want to use an auto-encoder? For someone only just exposed to Machine Learning and focusing mostly in prediction or classification tasks, they might lean towards trying more complex and flexible models, therefore it might seem counter intuitive to try to reduce dimensionality of the problem. However there are perks to compressing the data into a smaller dimension, and one can see this idea of autoencoder architecture as a non linear extension to PCA. By taking "most of the information", we can leave out noisy channels, and help reconstruction (see Information Retrieval). But by compressing the problem, we can also learn useful representations: a more straightforward example of useful representation learning is the NMF (non negative matrix factorization) application to facial recognition where reconstruction can be achieved learning "parts" of the face (mouth, eyebrow, nose...) instead of each pixel. This allows less noise injection than pixel-by-pixel methods and less sensitivity towards randomized attacks (if you flip pixels one by one at random in the training set). This makes it a powerful tool to unlock patterns as well as to generate patterns. 

If variational autoencoders have architectural similarities to deterministic autoencoders, the theoretical problem is in fact very different. VAE belong to a class of Bayesian variational methods. What do variational methods do? 
Assuming that our observed variable $x$ can be modeled through a latent (unknown) process $z$, and parameters $\theta$, we model the **relationship** between those variables through their probabilistic **distributions**. 

In a Bayesian setting, one might assume $z \sim p_{\theta}(z)$ prior and observe $p_{\theta}(x|z)$ the likelihood of the data, then the posterior of our data generating process is $p_{\theta}(z|x) = \frac{p_{\theta}(x,z)}{p_{\theta}(x)} = \frac{p_{\theta}(x,z)}{\int p_{\theta}(x,z) dz} $. Then the posterior might not be computationally tractable (it's hard to integrate quantities!).

![Variational inference approximation of posterior distribution](./images/bp-VI.png)
<img src='./images/bp-VI.png' alt='Variational inference approximation of posterior distribution'/>
Variational inference  tries to approximate the posterior through an optimisation framework where we sift through a family of flexible enough distributions to get as close to the true posterior as we can. This notion of distance between two distributions is given by the KL divergence : $\min_{q_\psi \in \mathcal{Q}} KL(q_\psi(z)||p_\theta(z|x))$. By some computations, we can rewrite this minimisation problem as maximising a bound called ELBO (evidence lower bound) that can be interpreted as maximising the expected log likelihood (learn from what we see) while minimising the distance between posterior to prior (we do not want to diverge from the prior too much).  
 Many methods derive from this principle:
 - stochastic variational inference (**SVI**): use a stochastic approximation for the ELBO to simplify the objective (faster to compute) by taking a random minibatch $B$ from the dataset and $ELBO((\psi_n)_{n}, \theta) \approx \frac{N}{B} \sum_{x_n \in B} ELBO(\psi_n, \theta)$. 
 - amortized variational inference (**amortized VI**): instead of optimising to find $\psi_n$ at each $n$ in the minibatch, we use an *inference network* that is trained to predict $\psi_n$ from $x_n$ such that $\psi_n = f_\phi^{inf}(x_n)$. This can be combined with SVI. This is the framework in which VAE belong mostly.
 - gradient-based VI: if $q$ is chosen so that the ELBO is differentiable then we can use MC samples after pushing gradients into expectation to estimate the gradients and minimise through SGD.
 - reparametrized VI: as an extension of the gradient-based idea, we reparametrise the $q$ so that we can differentiate easily on the parameter and sample independently therefore gradients can be propagated back since we can switch gradient/expectation. 
 - mean field VI: Gaussian based, sometimes can be analytically optimised sometimes using rVI...
 - coordinate ascent VI, ADVI and more (see Murphy)
 
# Variational Auto Encoders

![Deterministic Autoencoder Architecture](./images/bp-VAE.png)
<img src='./images/bp-VAE.png' alt='Deterministic Autoencoder Architecture'/>
## In Theory


Aren't headings cool?


## In Practice
======
